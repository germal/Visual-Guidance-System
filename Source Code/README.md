# Guidance system for visually impaired human
### Current Status

- Semantic segmentation is done using ResNet18dilated + C1_deepsup architecture.
- Network is retrained for 12 classes with 11 images(data/corridor_dataset/images).
- Rtabmap ROS implimentaion is used and integrated with move_base package for path planning.
- Points in the point cloud generated by Rtabmap is labelled with semantic information.
- Using the segmentation images and depth map, a voice guided program is made.

### Installation of software
- First setup the Jetson Xavier using the "SDKManager" and install all jetson SDK components.
- Install Zed SDK for Jetson(Follow the website)
https://www.stereolabs.com/developers/nvidia-jetson/

- Install ROS melodic
http://wiki.ros.org/melodic/Installation/Ubuntu

- Install ZED-ROS-Wrapper
https://github.com/stereolabs/zed-ros-wrapper

- Install cv_bridge from source
https://github.com/ros-perception/vision_opencv/tree/melodic/cv_bridge
Set the following variables while calling cmake
Note: Path and file name may differ
>cmake 
-DPYTHON_EXECUTABLE=/usr/bin/python3
-DPYTHON_INCLUDE_DIR=/usr/include/python3.7m 
-DPYTHON_LIBRARY=/usr/lib/aarch64-linux-gnu/libpython3.6m.so ../

- For python3 setup virtual environment and link it with the OpenCV
https://docs.python.org/3/library/venv.html
(refer to point 13) https://devtalk.nvidia.com/default/topic/1042035/jetson-agx-xavier/installing-opencv4-on-xavier-solved-/

- Setup the "semantic-segmentation-pytorch" from the "Source Code"
Install python packages - pytorch, scipy(v1.1.0), numpy, torchvision, yacs, tqdm
 Note: Newer versions of scipy have some issue.

- From "Source Code" build the Rtabmap package
https://github.com/introlab/rtabmap/wiki/Installation (Follow the instructions)

- From "Source Code" build Rtabmap_ros
Put the rtabmap_ros && visguide package in your ros workspace and run "catkin_make"

- Build the zed_code package
>cd zed_code/build
>cmake ..
>make

### Usage

1. Activate your virtual env
> source /path to virtual env/bin/activate

2. Run semantic segmentaion(make sure that the topic name is same as it is given in rtabmap.launch file in visguide package)
> cd /PATH_to_semantic-segmentation-pytorch
>python sample_Resnet18.py

3. To run the rtabmap and zed camera 
> roslaunch visguide visguide.launch

Note: To run the rtabmap and zed camera seperately run
> roslaunch visguide rtabmap.launch
> roslaunch visguide zedm.launch

4. To run the segmentation image based voice interface run
Step 1
Step 2
> roslaunch visguide zedm.launch
> rosrun visguide visguide_node

5. For visualisation
>rviz

Note: point_cloud topic - /rtabmap/map_cloud

### Corridor_Dataset
- visguide/data.svo
- visguide/data_30fps.svo

- To collect the data
>cd zed_code/build
>./ZED_Save_depth ../data/data.svo

### Training
>python train.py --gpus 0 --arch_encoder resnet18dilated --arch_decoder c1_deepsup --fc_dim 512 
--list_train data/train.odgt --list_val data/train.odgt --num_class 12 
--weights_encoder baseline-resnet18dilated-c1_deepsup/encoder_epoch_20.pth 
--weights_decoder baseline-resnet18dilated-c1_deepsup/decoder_epoch_20.pth 
--num_epoch 20 --lr_decoder 2e-4 --weight_decay 1e-6 --epoch_iters 333 --imgSize 376
--imgMaxSize 672

